{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "* When fundraising: it's AI  \n",
    "* When recruiting: it's machine learning  \n",
    "* When implementing: it's linear regression  \n",
    "\n",
    "## Understanding Linear Regression  \n",
    "\n",
    "Linear regression is a mathematical technique used to predict an outcome based on a set of input variables. It works by assigning a specific weight to each input and summing their effects to generate a final predicted value. This process is known as a **linear combination**.  \n",
    "\n",
    "### A Real-World Analogy: Grocery Shopping  \n",
    "\n",
    "Imagine you're shopping for groceries and purchase `2.5 lbs` of potatoes, `1 lb` of carrots, and `2 bottles` of milk. If potatoes cost `$2 per lb`, carrots `$4 per lb`, and each bottle of milk is `$3`, the total expense is calculated as:  \n",
    "\n",
    "\n",
    "$$\n",
    "(2.5 \\times 2) + (1.0 \\times 4) + (2 \\times 3) = 15\n",
    "$$\n",
    "\n",
    "This demonstrates the principle of linearityâ€”each item's contribution to the total is proportional to its price, and any increase in quantity leads to a predictable increase in cost.\n",
    "\n",
    "> **Linearity** means that for every fixed change in an input variable, the predicted outcome changes by a constant amount.\n",
    "\n",
    "\n",
    "### The Role of Weights (Coefficients)  \n",
    "\n",
    "In statistical modeling, the prices in the example above represent **coefficients** or **weights**â€”values that determine how much each input influences the final outcome.  \n",
    "\n",
    "Linear regression is a method under the broader category of **regression analysis**, **which focuses on predicting numerical values based on input features**. For instance, a laptop pricing model may estimate the value of a laptop using attributes such as its processor speed, size of harddrive, and weight.  \n",
    "\n",
    "### Features and Outputs  \n",
    "\n",
    "We refer to the input data used in predictions as **features** or **inputs**, while the predicted result is called the **output** or **response**. In our laptop example, a laptop's hard drive capacity is an input, and its estimated market price is the output. The goal of regression is to fine-tune the model so that its predictions align as closely as possible with actual values.  \n",
    "\n",
    "### How Predictions Are Made  \n",
    "\n",
    "Linear regression assumes that changes in an input variable correspond to proportional changes in the predicted output. For example, if a laptop is initially estimated at `$1,000`, and increasing its memory size by `1 GB` consistently adds `$20` to its value, then:  \n",
    "\n",
    "- A `8 GB` increase raises the estimate by `$160`.  \n",
    "- A `16 GB` increase raises it by `$320`.  \n",
    "\n",
    "Similarly, other factors contribute positively or negatively. For example, using a higher generation processor increase the predicted price by `$300` per generation, whereas increasing the weight of the laptop decrease the price by a fixed amount per pound.  \n",
    "\n",
    "This structured relationship between inputs and outputs makes linear regression a valuable tool for making predictions in various fields, from finance to healthcare and beyond.  \n",
    "\n",
    "## Shopping Bill\n",
    "\n",
    "$$\n",
    "Bill Totals: \\$2 \\times 2.5 + \\$4 \\times 1.0 + \\$3 \\times 2 = \\$15\n",
    "$$\n",
    "\n",
    "$$\n",
    "total = c_1 \\times x_1 + c_2 \\times x_2 + c_3 \\times x_3\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ½Exercise 1\n",
    "Write a program that estimates multiple laptop values and outputs each estimation on a new line. The `x` values are contained in `X` and coefficients are contained in `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# input values for three laptops: processor speed, size of harddrive (TB), weight, size of memory (GB), processor generation\n",
    "X = [\n",
    "        [2.5, 0.5,  1.8,  8, 2], # Mid-range laptop\n",
    "        [3.2, 1,    2.2, 16, 3], # High-end laptop\n",
    "        [1.8, 0.25, 3,  4, 1]  # Budget laptop\n",
    "    ]\n",
    "c = [200, 50, -200, 40, 300]   # coefficient values\n",
    "\n",
    "def predict(X, c):\n",
    "    price = 0\n",
    "    print(price)\n",
    "\n",
    "predict(X, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output was\n",
    "<pre>\n",
    "1085.0\n",
    "1790.0\n",
    "232.5\n",
    "</pre>\n",
    "you got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Makes It Easier! \n",
    "\n",
    "When working with numerical data in Python, **NumPy** is an essential tool. It provides a powerful array structure and optimized operations for handling large datasets efficiently. Instead of manually implementing matrix calculations, NumPy allows us to simplify these computations significantly.  \n",
    "\n",
    "### What is NumPy?  \n",
    "\n",
    "NumPy (**Numerical Python**) is a core library for scientific computing. It provides:  \n",
    "- **Multidimensional arrays** for efficient data storage and manipulation  \n",
    "- **Mathematical and statistical functions** for complex calculations  \n",
    "- **Linear algebra utilities**, including matrix operations  \n",
    "- **Random number generation** for simulations  \n",
    "- **File I/O functions** to handle datasets easily  \n",
    "\n",
    "With NumPy, we can efficiently handle large-scale computations that would otherwise be slow using basic Python lists.  \n",
    "\n",
    "### Matrix Multiplication in NumPy  \n",
    "\n",
    "NumPy offers various ways to perform matrix multiplication, an essential operation in linear regression:  \n",
    "\n",
    "1. **`np.matmul(A, B)`** â€“ Performs matrix multiplication  \n",
    "2. **`A @ B`** â€“ A shorthand for `matmul`, often used for readability  \n",
    "3. **`np.dot(A, B)`** â€“ Computes the dot product, which in some cases behaves similarly to `matmul`  \n",
    "\n",
    "While `matmul` and `@` are used for general matrix multiplication, `dot` is specifically designed for dot products, and the two are **not always interchangeable** for higher-dimensional arrays.  \n",
    "\n",
    "### Simplifying the Regression Calculation  \n",
    "\n",
    "Instead of manually summing up each featureâ€™s contribution, we can use NumPyâ€™s `@` operator or `matmul` to compute a prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature matrix: Each row represents an object (e.g., laptops) and each column a feature\n",
    "x = np.array([2.5, 0.5,  1.8,  8, 2])\n",
    "\n",
    "# Coefficient values for each feature\n",
    "c = np.array([200, 50, -200, 40, 300])\n",
    "\n",
    "# Compute predictions using matrix multiplication\n",
    "# c1x1 + c2x2 + ... + c5x5\n",
    "prediction = x @ c  # Equivalent to np.matmul(X, c)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power of Linear Algebra in NumPy  \n",
    "\n",
    "One of the greatest advantages of **linear algebra**â€”the mathematical foundation of many machine learning techniquesâ€”is its ability to perform computations efficiently using arrays (also known as **vectors** and **matrices** in mathematical terms). Instead of performing calculations one by one, **linear algebra allows us to process multiple cases simultaneously**.  \n",
    "\n",
    "### Applying Matrix Multiplication to Multiple Cases  \n",
    "\n",
    "Instead of computing predictions individually for each set of input values, we can store all of them in a **two-dimensional NumPy array**. When we apply matrix multiplication (`@`), NumPy will automatically compute the predictions for all cases at once.\n",
    "\n",
    "#### Example: Predicting Multiple Values at Once  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1085.  1790.   232.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature matrix: Each row represents a case, and each column represents a feature\n",
    "X = np.array([\n",
    "        [2.5, 0.5,  1.8,  8, 2], # Mid-range laptop\n",
    "        [3.2, 1,    2.2, 16, 3], # High-end laptop\n",
    "        [1.8, 0.25, 3,  4, 1]  # Budget laptop\n",
    "    ])\n",
    "c = np.array([200, 50, -200, 40, 300])   # coefficient values\n",
    "\n",
    "# Perform matrix multiplication to get predictions for all cases at once\n",
    "# calculates c1x1 + c2x2 + ... + c5x5 per sub-array of X.\n",
    "predictions = X @ c  # Equivalent to np.matmul(X, c)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Why is This Approach So Powerful?  \n",
    "\n",
    "- **Efficiency** â€“ Instead of using loops to compute each prediction separately, we compute all predictions in one step.  \n",
    "- **Scalability** â€“ Works seamlessly for thousands or even millions of cases.  \n",
    "- **Cleaner Code** â€“ Makes machine learning models more readable and maintainable.  \n",
    "\n",
    "By leveraging NumPyâ€™s array operations, we move closer to how real-world machine learning models handle vast datasetsâ€”quickly and efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Where Linear Regression Shines  \n",
    "\n",
    "So far, we have manually estimated laptop prices using predefined coefficients. However, in real-world scenarios, **we often donâ€™t know these coefficients**â€”we need to **discover them from data**.  \n",
    "\n",
    "Imagine we have a dataset with specifications (processor speed, RAM, storage, etc.) and the actual selling prices of multiple laptops. Our goal is to determine how much each feature contributes to the price. **Linear regression allows us to learn these relationships automatically.**  \n",
    "\n",
    "## Least Squares Method: Finding the Best Coefficients  \n",
    "\n",
    "The **Least Squares Method**, developed by **Legendre and Gauss** in the early 1800s, is the foundation of linear regression.  \n",
    "\n",
    "It helps us find the **best possible coefficients** (\\( $w_1$, $w_2$, ... $w_n$ \\)) so that our predicted prices ( $\\hat{y}$ ) match the real prices ( $y$ ) as closely as possible. The model takes the form:  \n",
    "\n",
    "$$\n",
    "\\hat{y} = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( $x_i$ \\) are the laptopâ€™s features (e.g., processor speed, memory).  \n",
    "- \\( $w_i$ \\) are the coefficients we are trying to find.  \n",
    "- \\( $b$ \\) is the **intercept** (base price when all features are 0).  \n",
    "\n",
    "\n",
    "### **Why \"Least Squares\"?**  \n",
    "The name comes from the way we measure how good our predictions are. The method **minimizes the sum of squared errors** between predicted and actual prices:  \n",
    "\n",
    "$$\n",
    "\\sum (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "where \n",
    "- $\\hat{y}$ predicted prices \n",
    "- $y$ the real prices \n",
    "\n",
    "1. **Why do we square the errors?**  \n",
    "   - Prevents negative and positive differences from canceling out.  \n",
    "   - Gives more weight to large errors, improving accuracy.  \n",
    "\n",
    "2. **Why not just use simple averages?**  \n",
    "   - The least squares method doesnâ€™t just compute averagesâ€”it **finds the best-fit line in multi-dimensional space** to **minimize total error**.  \n",
    "\n",
    "\n",
    "## Why This Matters  \n",
    "\n",
    "- **No need for manual tuning** â€“ The model automatically learns the best coefficients.  \n",
    "- **More accurate predictions** â€“ Reduces errors by fitting the best possible line.  \n",
    "- **Scales to large datasets** â€“ Works efficiently with thousands of laptops.  \n",
    "\n",
    "\n",
    "\n",
    "## Implementing Least Squares in Python  \n",
    "\n",
    "Luckily, Python has built-in tools like **NumPy** and **Scikit-Learn** to handle this easily. Instead of solving for coefficients manually, we can use simple functions to do the work for us.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ½Exercise 2: Finding the Best Set of Coefficients  \n",
    "\n",
    "In this exercise, we will calculate the **squared error** for multiple sets of coefficient values and determine which set gives the best fit. This is a **simplified version of the least squares method**, where instead of finding the perfect coefficients, we test a fixed set of possible coefficient combinations and choose the one with the smallest error.  \n",
    "\n",
    "\n",
    "### **Task**  \n",
    "Write a program that:  \n",
    "- Calculates the **squared error** for different coefficient sets  \n",
    "- Identifies the **best coefficient set** (i.e., the one with the smallest squared error)  \n",
    "\n",
    "\n",
    "### **Dataset: Laptop Prices**  \n",
    "\n",
    "We will use a dataset where each row represents a **laptop** and each column represents a **feature**:  \n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Processor Speed (GHz)** | How fast the processor runs |\n",
    "| **Hard Drive Size (GB)** | Total storage capacity |\n",
    "| **Weight (kg)** | Laptop weight (lighter is often better) |\n",
    "| **Memory (GB)** | Amount of RAM |\n",
    "| **Processor Generation** | Age and performance tier of the CPU |\n",
    "\n",
    "Each laptop also has a **real market price** (target variable \\($y$\\)), which we will compare against our modelâ€™s predicted prices.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1: Squared Error =            0.00\n",
      "\n",
      "Set 2: Squared Error =            0.00\n",
      "\n",
      "Set 3: Squared Error =            0.00\n",
      "\n",
      "Set 4: Squared Error =            0.00\n",
      "\n",
      "Set 5: Squared Error =            0.00\n",
      "\n",
      "\n",
      "Best coefficient set: Set 1 with SSE =            0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Laptop dataset: Each row is a laptop, each column is a feature\n",
    "\n",
    "# Processor Speed (GHz)\n",
    "# Hard Drive Size (GB)\n",
    "# Weight (kg)\n",
    "# Memory (GB)\n",
    "# Processor Generation\n",
    "\n",
    "X = np.array([\n",
    "    [2.5, 512, 1.8, 8, 2],   # Laptop 1\n",
    "    [3.2, 1024, 2.2, 16, 3], # Laptop 2\n",
    "    [1.8, 256, 1.5, 4, 1]     # Laptop 3\n",
    "])\n",
    "\n",
    "# Actual laptop prices\n",
    "y = np.array([1200, 2200, 800])  # Market prices in dollars\n",
    "\n",
    "# Possible coefficient values to evaluate\n",
    "candidates = np.array([\n",
    "    [10, 0.1, -5, 20, 50],  # Coefficient set 1\n",
    "    [50, 0.2, -10, 50, 100],  # Coefficient set 2\n",
    "    [100, 0.3, -20, 100, 150],  # Coefficient set 3\n",
    "    [250, 0.4, -80, 150, 250],   # Coefficient set 4\n",
    "    [400, 0.8, -150, 200, 350]   # Coefficient set 5\n",
    "])\n",
    "\n",
    "def squared_error(X, y, c):\n",
    "    \"\"\"Computes the sum of squared errors (SSE) for a given coefficient set.\"\"\"\n",
    "    # Compute predictions here.\n",
    "    return 0 # Placeholder for now\n",
    "\n",
    "# Find the best coefficient set\n",
    "best_index = None\n",
    "best_error = float('inf')\n",
    "\n",
    "for i, c in enumerate(candidates):\n",
    "    error = squared_error(X, y, c)\n",
    "    print(f\"Set {i+1}: Squared Error = {error:>15,.2f}\\n\")\n",
    "\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_index = i\n",
    "\n",
    "print(f\"\\nBest coefficient set: Set {best_index+1} with SSE = {best_error:>15,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output is somewhat like this you're correct.\n",
    "<pre>\n",
    "Predictions: [327.2 593.4 166.1]\n",
    "Set 1: Squared Error =    3,744,772.61\n",
    "\n",
    "Predictions: [ 809.4 1442.8  426.2]\n",
    "Set 2: Squared Error =      865,646.64\n",
    "\n",
    "Predictions: [1467.6 2633.2  776.8]\n",
    "Set 3: Squared Error =      259,810.24\n",
    "\n",
    "Predictions: [2385.8 4183.6 1282.4]\n",
    "Set 4: Squared Error =    5,573,500.36\n",
    "\n",
    "Predictions: [3439.6 6019.2 1849.8]\n",
    "Set 5: Squared Error =   20,704,176.84\n",
    "\n",
    "\n",
    "Best coefficient set: Set 3 with SSE =      259,810.24\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Now it's time to find coefficients that minimize the sum of squared errors (SSE) using least squares method.\n",
    "\n",
    "In order to achieve this we need a new function from numpy called np.linalg.lstsq. This function takes the feature matrix X and the target values y as input, and returns the optimal coefficients c that minimize the sum of squared errors (SSE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linalg.lstsq(a, b, rcond=None)\n",
    "\n",
    "Official documentation from numpy.org, check the example and the curve fitting they have.\n",
    "\n",
    "https://numpy.org/doc/2.1/reference/generated/numpy.linalg.lstsq.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptop Specs and modeling their price and estimating\n",
    "\n",
    "|                 | proc. spd. (ghz) | hdd (gb) | weight | memory (gb) | proc. generation | price in $ (output) |\n",
    "|-----------------|-----------------:|---------:|-------:|------------:|-----------------:|--------------------:|\n",
    "| Budget Student  | 1.5              |      250 |   2.2  |        8    |           2      |               460   |\n",
    "| Office Worker   | 2.2              |      500 |   1.8  |       16    |           3      |               880   |\n",
    "| Developer       | 2.8              |     1000 |   1.5  |       32    |           4      |             1,370   |\n",
    "| Content Creator | 3.2              |     2000 |   1.3  |       64    |           5      |             2,010   |\n",
    "| Gamer           | 3.6              |     4000 |   2.0  |       64    |           6      |             2,340   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient estimates:  [ 146.31    0.08 -100.      9.35  182.96]\n",
      "Set of predicted prices for the five laptops:  [ 460.  880. 1370. 2010. 2340.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([\n",
    "    [1.5,  250, 2.2,  8,  2],  # Budget Student\n",
    "    [2.2,  500, 1.8, 16,  3],  # Office Worker\n",
    "    [2.8, 1000, 1.5, 32,  4],  # Developer\n",
    "    [3.2, 2000, 1.3, 64,  5],  # Content Creator\n",
    "    [3.6, 4000, 2.0, 64,  6]   # Gamer\n",
    "])\n",
    "\n",
    "y = np.array([ 460, 880, 1370, 2010, 2340])\n",
    "\n",
    "coefficients, _, _, _ = np.linalg.lstsq(X, y)\n",
    "np.set_printoptions(suppress=True, precision=2)  # suppress scientific notation, 2 decimal places\n",
    "print('Coefficient estimates: ', coefficients)\n",
    "print('Set of predicted prices for the five laptops: ', X @ coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's first look at the coefficient estimates. The value `146.31` of the first coefficient means that each GHz increase in the size adds `$146.31` to the price.\n",
    "- The value `-100` of the third coefficient means for each kg heavier the laptop is, the price of the laptop goes down `$100`, or conversely for each kg the laptop is lighter, you have to pay `$100` more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation to the unexpected perfect predictions in these five cases is that the model is always able to perfectly match the output values used as the data if the number of cases is less than or equal to the number of coefficients in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ½Last exercise fitting coefficient values with least squares:\n",
    "The output of the program should be the estimated coefficients and the predicted or \"fitted\" prices for the same set of laptops used to estimate the parameters. So if you fit the model using data for six laptops with known prices, the program will print out the prices that the model predicts for those six laptops (even if the actual prices are already given in the data).\n",
    "\n",
    "Hint:  \n",
    "`X = matrix[:, :-1]` â†’ Selects all rows and all but the last column.  \n",
    "`y = matrix[:, -1]` â†’ Selects all rows and only the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "matrix = np.array([\n",
    "    [1.5,  250, 2.2,  8,  2, 460],\n",
    "    [2.2,  500, 1.8, 16,  3, 880],\n",
    "    [2.8, 1000, 1.5, 32,  4, 1350],\n",
    "    [3.2, 2000, 1.3, 64,  5, 2000],\n",
    "    [3.6, 4000, 2.0, 64,  6, 2350],\n",
    "    [2.2,  500, 1.3, 16,  4, 1000],\n",
    "    [4.0, 4000, 4.0, 64,  6, 2250]\n",
    "])\n",
    "np.set_printoptions(precision=1)    # this just changes the output settings for easier reading\n",
    "\n",
    "def fit_model(matrix):\n",
    "    # Please write your code inside this function\n",
    "\n",
    "    # fit the matrix that's passed. the values below are placeholder values\n",
    "    c = np.asarray([])  # coefficients of the linear regression\n",
    "    x = np.asarray([])  # input data to the linear regression\n",
    "\n",
    "    print('Coefficient estimates: ', c)\n",
    "    print('Set of predicted prices for the five laptops: ', x @ c)\n",
    "\n",
    "# simulate reading a file\n",
    "fit_model(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to get something like this you got it right.\n",
    "<pre>\n",
    "Coefficient estimates:  [ 200   50 -200   40  300]\n",
    "Set of predicted prices for the five laptops:  1085.0\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
